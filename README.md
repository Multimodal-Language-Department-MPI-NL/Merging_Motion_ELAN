# Merging MediaPipe with ELAN Annotations

Merge MediaPipe keypoint time series with ELAN annotation data for multimodal gesture analysis.

## ğŸ”¬ Research Context

This module enables temporal alignment of gesture motion data with linguistic annotations, supporting multimodal communication research that examines the relationship between speech and gesture.

## ğŸ¯ What This Project Does

1. **Load multimodal data**: Read MediaPipe keypoint CSVs and ELAN annotation files
2. **Temporal alignment**: Map ELAN annotations to corresponding time points in motion data
3. **Data merging**: Combine motion trajectories with linguistic annotations
4. **Export merged datasets**: Save aligned multimodal data for downstream analysis

## ğŸ“Š Merging Process

1. **Load Data Sources**: Import MediaPipe keypoints and ELAN annotation files
2. **Inspect Formats**: Examine data structures and time ranges
3. **Temporal Mapping**: Align annotation time windows with motion data timestamps
4. **Merge Datasets**: Combine motion and annotation data by time
5. **Export Results**: Save merged multimodal datasets

## ğŸ”§ Methods

- **Time-based alignment**: Map ELAN begin/end times to MediaPipe timestamps
- **Annotation propagation**: Assign annotation labels to corresponding motion frames
- **Multi-file processing**: Handle multiple participants/sessions
- **Data validation**: Ensure temporal consistency across modalities

## ğŸ“ Project Structure

```
Merging_Motion_ELAN/
â”œâ”€â”€ README.md
â”œâ”€â”€ environment.yml
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ Multimodal_Merging.ipynb
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ processed/                     # Processed motion data
â”‚   â””â”€â”€ raw/                          # Raw input files
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py                       # Main processing script
â”‚   â””â”€â”€ utils.py                      # Utility functions
â””â”€â”€ tests/
    â”œâ”€â”€ __init__.py
    â””â”€â”€ test_processing.py
```

## ğŸš€ Quick Start

### Prerequisites

```bash
conda env create -f environment.yml
conda activate merging
```

### Run the Interactive Notebook

```bash
jupyter lab
# Open notebooks/Multimodal_Merging.ipynb
```

### Basic Usage

1. Place MediaPipe keypoint CSVs in `data/processed/`
2. Place ELAN annotation files in `data/raw/`
3. Run the merging notebook to align and combine datasets
4. Export merged data for multimodal analysis

## ğŸ“ˆ Data Format

### Input
- **MediaPipe CSVs**: Time series with keypoint coordinates (X, Y, Z) and timestamps
- **ELAN annotations**: Tab-separated files with begin/end times and annotation labels

### Output
- **Merged CSVs**: Combined datasets with motion data and corresponding annotations
- **Aligned time series**: Motion trajectories with linguistic labels at each time point

## ğŸ”— Related Projects

- `../MediaPipe_keypoints_extraction/`
- `../Smoothing/`
- `../Normalization/`
- `../Speed_Acceleration_Jerk/`

## ğŸ“– References

- ELAN documentation: [https://archive.mpi.nl/tla/elan](https://archive.mpi.nl/tla/elan)
- Multimodal gesture analysis literature

## ğŸ¤ Contributing

This project is part of the MPI Multimodal Interaction Research framework. For questions or contributions, please refer to the main project documentation.

## ğŸ“„ License

This project is part of the MPI research framework. Please refer to the main project license for usage terms.